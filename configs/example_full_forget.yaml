main: 
  exp_name: train
  pretrain_model: aix_coder
  template: llamacode 
  use_wandb: false
  save_dir: /mnt/bd/life-llm-factory-volume/life_llm_factory/forget_learning_save_models_aixcoder/code_leak_4qa_w_code_common_knowledge/forget_loss_forget_arise
  pretrain_model_dir: /mnt/bd/life-llm-factory-volume/aixcoder
  
accelerate_config:
  debug: false
  compute_environment: LOCAL_MACHINE
  rdzv_backend: static
  same_network: true
  gpu_ids: all
  mixed_precision: fp16
  distributed_type: DEEPSPEED # DEEPSPEED MULTI_GPU 
  deepspeed_config:
    zero_stage: 3
    zero3_init_flag: true
    offload_optimizer_device: none
    offload_param_device: none

model_config:
  stage: forget
  do_train: true
  overwrite_output_dir: true

  output_dir: ${main.save_dir}/${main.exp_name}
  model_name_or_path: ${main.pretrain_model_dir}/${main.pretrain_model}

  template: ${main.template}
  finetuning_type: full

  dataset: code_leak_4qa_w_code_common_knowledge
  dataset_dir: data
  cutoff_len: 512
  val_size: 0.0001 

  logging_steps: 1
  save_steps: 30
  eval_steps: 10
  plot_loss: true

  lr_scheduler_type: cosine
  learning_rate: 1.0e-5
  num_train_epochs: 1
  max_steps: 40
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 1

  evaluation_strategy: 'no'
  ddp_find_unused_parameters: false
  load_best_model_at_end: false
  fp16: true
  log_on_each_node: false 

  use_wandb: ${main.use_wandb}
  wandb_project: code_llm_sft # wandb
  wandb_entity: code_llm_sft
  wandb_name: ${main.exp_name}
  report_to: none 
