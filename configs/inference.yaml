model_name_or_path: ${your model path}
adapter_name_or_path: null
template: qwen
inference_paths: ${your inference file path}
inference_save_dir: ${your save path}
batch_size: 16
cut_len: 500000  # 对于数据集太长的，进行截断，增加评测的速度
generate_config_path: null
use_vllm: true
do_sample: false
tp_size: 1 # vllm的tensor parallel size
